# Assignment 03

# Pytorch tutorial session

[Google Colaboratory](https://colab.research.google.com/drive/1rqPRnglwIoPjIcAHSatPjTeXpaIMoj4Q?usp=sharing)

A basic introduction to pytorch and follow a task: Word Window Classification

dectect the Name of location (one-word only, multiple words is not considered to be a location such as San Francisco)

# Dependency Parsing

## Materials

[Hand-out](Assignment%2003%2059c6235cac224f24b8f93684350a42a9/Hand-out%204e9d88facee74ef8908cc52e5ee604e3.md)

[Slide](Assignment%2003%2059c6235cac224f24b8f93684350a42a9/Slide%206224351b295242b697721f86dae88888.md)

[Slide-demo](Assignment%2003%2059c6235cac224f24b8f93684350a42a9/Slide-demo%2003181cac872b4b10996488cd329bac95.md)

## Coding assignment

![Untitled](Assignment%2003%2059c6235cac224f24b8f93684350a42a9/Untitled.png)

### Transition-based parser

At every step it maintains a partial parse, which is represented as follows:

- A stack of words that are currently being processed.
- A buffer of words yet to be processed.
- A list of dependencies predicted by the parser.

Initially, the stack only contains ROOT, the dependencies list is empty, and the buffer contains all words of the sentence in order. At each step, the parser applies a transition to the partial parse until its buffer is empty and the stack size is 1. The following transitions can be applied:

- SHIFT: removes the first word from the buffer and pushes it onto the stack.
- LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack, adding a first word → second word dependency to the dependency list.
- RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack, adding a second word → first word dependency to the dependency list.

```python
class PartialParse(object):
		def __init__(self, sentence):
        """Initializes this partial parse.

        @param sentence (list of str): The sentence to be parsed as a list of words.
                                        Your code should not modify the sentence.
        """
        # The sentence being parsed is kept for bookkeeping purposes. Do NOT alter it in your code.
        self.sentence = sentence
        self.stack = ["ROOT"]
        self.buffer = sentence[:]
        self.dependencies = []

    def parse_step(self, transition):
        """Performs a single parse step by applying the given transition to this partial parse

        @param transition (str): A string that equals "S", "LA", or "RA" representing the shift,
                                left-arc, and right-arc transitions. You can assume the provided
                                transition is a legal transition.
        """
        if transition == "S":
            self.stack.append(self.buffer.pop(0))
        elif transition == "LA":
            dependent = self.stack.pop(-2)
            head = self.stack[-1]
            self.dependencies.append((head, dependent))
        else:
            dependent = self.stack.pop(-1)
            head = self.stack[-1]
            self.dependencies.append((head, dependent))
		def parse(self, transitions):
        """Applies the provided transitions to this PartialParse

        @param transitions (list of str): The list of transitions in the order they should be applied

        @return dependencies (list of string tuples): The list of dependencies produced when
                                                        parsing the sentence. Represented as a list of
                                                        tuples where each tuple is of the form (head, dependent).
        """
        for transition in transitions:
            self.parse_step(transition)
        return self.dependencies
```

---

### Algorithm to predict parsing dependency of a minibatch of data by predicting which transition should be applied next to a partial parse

![Untitled](Assignment%2003%2059c6235cac224f24b8f93684350a42a9/Untitled%201.png)

```python
def minibatch_parse(sentences, model, batch_size):
    """Parses a list of sentences in minibatches using a model.

    @param sentences (list of list of str): A list of sentences to be parsed
                                            (each sentence is a list of words and each word is of type string)
    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function
                                model.predict(partial_parses) that takes in a list of PartialParses as input and
                                returns a list of transitions predicted for each parse. That is, after calling
                                    transitions = model.predict(partial_parses)
                                transitions[i] will be the next transition to apply to partial_parses[i].
    @param batch_size (int): The number of PartialParses to include in each minibatch

    @return dependencies (list of dependency lists): A list where each element is the dependencies
                                                    list for a parsed sentence. Ordering should be the
                                                    same as in sentences (i.e., dependencies[i] should
                                                    contain the parse for sentences[i]).
    """

		###     Implement the minibatch parse algorithm.  Note that the pseudocode for this algorithm is given in the pdf handout.
    ###
    ###     Note: A shallow copy (as denoted in the PDF) can be made with the "=" sign in python, e.g.
    ###                 unfinished_parses = partial_parses[:].
    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.
    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances
    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.
    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`
    ###             contains references to the same objects. Thus, you should NOT use the `del` operator
    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that
    ###             is being accessed by `partial_parses` and may cause your code to crash.
    dependencies = []

    partial_parses = []
    for sentence in sentences:
        partial_parses.append(PartialParse(sentence))
    unfinished_parses = partial_parses[:] #shallow copy

    while unfinished_parses != []:
        minibatch = unfinished_parses[:batch_size]
        while minibatch != []:
            transitions = model.predict(minibatch)
            for parser,transition in zip(minibatch,transitions):
                parser.parse_step(transition)
            minibatch = [parser for parser in minibatch if parser.buffer != [] or len(parser.stack) > 1]
        unfinished_parses = unfinished_parses[batch_size:]
    ### END YOUR CODE
    dependencies = [parser.dependencies for parser in partial_parses]
    return dependencies
```

### Model - Training

We are now going to train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next.
First, the model extracts a feature vector representing the current state. We will be using the feature set presented in the original neural dependency parsing paper: [A Fast and Accurate Dependency Parser using Neural Networks](https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf)

![Untitled](Assignment%2003%2059c6235cac224f24b8f93684350a42a9/Untitled%202.png)

```python
class ParserModel(nn.Module):
    def __init__(self, embeddings, n_features=36,
        hidden_size=200, n_classes=3, dropout_prob=0.5):
        """ Initialize the parser model.

        @param embeddings (ndarray): word embeddings (num_words, embedding_size)
        @param n_features (int): number of input features
        @param hidden_size (int): number of hidden units
        @param n_classes (int): number of output classes
        @param dropout_prob (float): dropout probability
        """
        super(ParserModel, self).__init__()
        self.n_features = n_features
        self.n_classes = n_classes
        self.dropout_prob = dropout_prob
        self.embed_size = embeddings.shape[1]
        self.hidden_size = hidden_size
        self.embeddings = nn.Parameter(torch.tensor(embeddings))

        W = nn.Parameter(torch.empty(n_features * self.embed_size, hidden_size))
        self.embed_to_hidden_weight = nn.init.xavier_uniform_(W)
        b1 = nn.Parameter(torch.empty(hidden_size))
        self.embed_to_hidden_bias = nn.init.uniform_(b1)
        # drop out
        self.dropout = nn.Dropout(p=dropout_prob)
        # hidden to logits weight and bias
        U = nn.Parameter(torch.empty(hidden_size, n_classes))
        self.hidden_to_logits_weight = nn.init.xavier_uniform_(U)
        b2 = nn.Parameter(torch.empty(n_classes)) 
        self.hidden_to_logits_bias = nn.init.uniform_(b2)

        self.relu = F.relu
        ### END YOUR CODE

    def embedding_lookup(self, w):
        """ Utilize `w` to select embeddings from embedding matrix `self.embeddings`
            @param w (Tensor): input tensor of word indices (batch_size, n_features)

            @return x (Tensor): tensor of embeddings for words represented in w
                                (batch_size, n_features * embed_size)
        """

        ### YOUR CODE HERE (~1-4 Lines)
        ### TODO:
        ###     1) For each index `i` in `w`, select `i`th vector from self.embeddings
        ###     2) Reshape the tensor using `view` function if necessary
        ###
        ### Note: All embedding vectors are stacked and stored as a matrix. The model receives
        ###       a list of indices representing a sequence of words, then it calls this lookup
        ###       function to map indices to sequence of embeddings.
        ###
        ###       This problem aims to test your understanding of embedding lookup,
        ###       so DO NOT use any high level API like nn.Embedding
        ###       (we are asking you to implement that!). Pay attention to tensor shapes
        ###       and reshape if necessary. Make sure you know each tensor's shape before you run the code!

        x = self.embeddings[w]
        shape = x.size()
        x = x.view(shape[0], shape[1]*shape[2])
        ### END YOUR CODE
        return x

    def forward(self, w):
        """ Run the model forward.

            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss

            PyTorch Notes:
                - Every nn.Module object (PyTorch model) has a `forward` function.
                - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor.
                    For example, if you created an instance of your ParserModel and applied it to some `w` as follows,
                    the `forward` function would called on `w` and the result would be stored in the `output` variable:
                        model = ParserModel()
                        output = model(w) # this calls the forward function
                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward

        @param w (Tensor): input tensor of tokens (batch_size, n_features)

        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)
                                 without applying softmax (batch_size, n_classes)
        """
        ### YOUR CODE HERE (~3-5 lines)
        ### TODO:
        ###     Complete the forward computation as described in write-up. In addition, include a dropout layer
        ###     as decleared in `__init__` after ReLU function.
        ###
        ### Note: We do not apply the softmax to the logits here, because
        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.
        ###
        ### Please see the following docs for support:
        ###     Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul
        ###     ReLU: https://pytorch.org/dcs/stable/nn.html?highlight=relu#torch.nn.functional.relu

        h = self.relu(torch.matmul(self.embedding_lookup(w), self.embed_to_hidden_weight) + self.embed_to_hidden_bias)
        h = self.dropout(h)
        logits = torch.matmul(h, self.hidden_to_logits_weight) + self.hidden_to_logits_bias
        ### END YOUR CODE
        return logits
```

![Untitled](Assignment%2003%2059c6235cac224f24b8f93684350a42a9/Untitled%203.png)

## Final result on train set and test set

Epoch 10 out of 10
100%██████████████████████████| 1848/1848 [01:25<00:00, 21.73it/s]
Average Train Loss: 0.06789379102456106
Evaluating on dev set
1445850it [00:00, 43526222.23it/s]

- dev UAS: 88.80
New best dev UAS! Saving model.

==============================================
TESTING

Restoring the best model weights found on the dev set
Final evaluation on test set
2919736it [00:00, 56781084.43it/s]

- test UAS: 89.34
Done!