# Assignment 01

# How can we represent words in a computer?

Unlike pictures, which are composed of pixels with value having range from 0-255 to represent the intensity of color, natural language are composed of sentences or letters with puntuaction. Computer can not directly work with this type of data, so it is necessary to have a method or technique to covert them into usable datatype that models can use and learn from.

⇒ **WORD EMBEDDING** is used to convert words into vectors.

## Naive approach - One hot encoding

As we know in most of classification problems, one of the popular ways to represent a category

is encode to array of 0s and 1s. If we consider each word in the dictionary is a distinct category.

![Untitled](Assignment%2001%2068d9059f69ed483396bdd410e5990993/Untitled.png)

### Drawback of one-hot vectors

- The numbers of vector are huge (equals to the number of words in the dictionary)
- All vectors are orthogonal to each other, so that we can not express the similarity of vectors (cosine of angle between 2 vectors motel and hotel $\frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|} \in [-1, 1]$ is always = 0)

## Word2vec

> *Word2Vec algorithm showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like “Stockholm” and “Sweden” have the same relationship between them as “Cairo” and “Egypt” have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between “had” and “has” is the same as that between “was” and “is”).*
> 

### Skip-gram model (implementation in exercise 2 coding assignment)

### Continuous bag of words -CBOW